num_results %>% str_extract("(?=News)")
num_results %>% str_extract("(?=News ).")
num_results %>% str_extract("(?=News ).*")
num_results %>% str_extract("(?=News ).*(?<=))")
num_results %>% str_extract("(?=News ).*(?<=)")
num_results %>% str_extract("(?=News )(.*)(?<=)")
num_results %>% str_extract("(?=News ).*(?=)")
gsub(".*News (.+) ).*", "\\1", num_results)
num_results
gsub(".*News (.+) ).*", "\\1", num_results)
str_extract(string = num_results, pattern = perl("(?<=News ).*(?=))"))
stringr::str_extract_all(num_results, "(?<=News ).+(?= ))" )
num_results
num_results <- unlist(num_results_elem$getElementText())
num_results
stringr::str_extract_all(num_results, "(?<=News ).+(?= ))" )
stringr::str_extract(num_results, "(?<=News ).+(?= ))" )
stringr::str_extract(num_results, "(?<=News ).+(?= \))" )
stringr::str_extract(num_results, "(?<=News ).+(?= \\))" )
num_results %>% str_extract("(?=News ).*(?=\\))")
num_results %>% str_extract("(?=News )+*(?=\\))")
num_results %>% str_extract("(?=News ).+(?=\\))")
num_results %>% str_extract("(?<=News ).+(?=\\))")
num_results %>% str_extract("(?<=News ().+(?=\\))")
num_results %>% str_extract("(?<=News ()).+(?=\\))")
num_results %>% str_extract("(?<=News \\().+(?=\\))")
num_results <- str_extract(num_results, "(?<=News \\().+(?=\\))")
num_results
num_results <- num_results %>% str_extract("(?<=News \\().+(?=\\))") %>% as.numeric()
num_results
num_results <- unlist(num_results_elem$getElementText())
num_results <- num_results %>% str_extract("(?<=News \\().+(?=\\))") %>% as.numeric()
num_results
num_results      <- num_results_elem$getElementText() %>%
unlist() %>%
str_extract("(?<=News \\().+(?=\\))") %>%
as.numeric()
num_results
### Do a search: ENTER search terms
url_search <- make_url(search_terms = "Tafel and (Essen or Lebensmittel)",
time_from    = "01/01/1900",
time_to      = "31/01/1900")
remDr$navigate(url_search)
num_results_elem <- remDr$findElement(using = "css", value = ".resultsHeader")
num_results_elem
num_results      <- num_results_elem$getElementText() %>%
unlist() %>%
str_extract("(?<=News \\().+(?=\\))") %>%
as.numeric()
num_results
crawl_year <- function(time_from, time_to) {
###
url_search <- make_url(search_terms = "Tafel and (Essen or Lebensmittel)",
time_from    = time_from,
time_to      = time_to)
remDr$navigate(url_search)
if (num_results < 11) {
if (num_results == 0) {
return(NULL)
}
page_source <- unlist(remDr$getPageSource())
html        <- read_html(page_source)
docids_lst  <- html %>% html_nodes(css = ".nexisuni-result-list input") %>% html_attr("data-docid") %>% str_replace("urn:contentItem:", "")
ids_lst     <- append(ids_lst, docids_lst)
} else {
elem_data_values <- remDr$findElements(using = "css", value = ".pagination .overflow .action")
page_max         <- unlist(lapply(elem, function(x) {x$getElementAttribute("data-value")})[[length(elem_data_values)]])
# Create empty list
ids_lst <- c()
for (i in 1:(page_max-1)) {
### After all 10 results
if (i > 1) {
button_next <- remDr$findElement(using = "css", "a.la-TriangleRight")
button_next$clickElement()
Sys.sleep(1)
}
## Get Ids for the search result
page_source <- unlist(remDr$getPageSource())
html        <- read_html(page_source)
docids_lst  <- html %>% html_nodes(css = ".nexisuni-result-list input") %>% html_attr("data-docid") %>% str_replace("urn:contentItem:", "")
ids_lst <- append(ids_lst, docids_lst)
print(i)
}
return(ids_lst)
}
}
crawl_year <- function(time_from, time_to) {
###
url_search <- make_url(search_terms = "Tafel and (Essen or Lebensmittel)",
time_from    = time_from,
time_to      = time_to)
remDr$navigate(url_search)
num_results_elem <- remDr$findElement(using = "css", value = ".resultsHeader")
num_results      <- num_results_elem$getElementText() %>%
unlist() %>%
str_extract("(?<=News \\().+(?=\\))") %>%
as.numeric()
if (num_results < 11) {
if (num_results == 0) {
return(c())
}
page_source <- unlist(remDr$getPageSource())
html        <- read_html(page_source)
docids_lst  <- html %>% html_nodes(css = ".nexisuni-result-list input") %>% html_attr("data-docid") %>% str_replace("urn:contentItem:", "")
ids_lst     <- append(ids_lst, docids_lst)
} else {
elem_data_values <- remDr$findElements(using = "css", value = ".pagination .overflow .action")
page_max         <- unlist(lapply(elem, function(x) {x$getElementAttribute("data-value")})[[length(elem_data_values)]])
# Create empty list
ids_lst <- c()
for (i in 1:(page_max-1)) {
### After all 10 results
if (i > 1) {
button_next <- remDr$findElement(using = "css", "a.la-TriangleRight")
button_next$clickElement()
Sys.sleep(1)
}
## Get Ids for the search result
page_source <- unlist(remDr$getPageSource())
html        <- read_html(page_source)
docids_lst  <- html %>% html_nodes(css = ".nexisuni-result-list input") %>% html_attr("data-docid") %>% str_replace("urn:contentItem:", "")
ids_lst <- append(ids_lst, docids_lst)
print(i)
}
return(ids_lst)
}
}
result <- crawl_year(time_from = "01/01/2000", time_to = "01/01/2001")
num_results_elem <- remDr$findElement(using = "css", value = ".resultsHeader")
elem_data_values <- remDr$findElements(using = "css", value = ".pagination .overflow .action")
page_max         <- unlist(lapply(elem, function(x) {x$getElementAttribute("data-value")})[[length(elem_data_values)]])
elem_data_values
elem_data_values <- remDr$findElements(using = "css", value = ".pagination .overflow .action")
elem_data_values <- remDr$findElements(using = "css", value = ".pagination .action")
page_max         <- unlist(lapply(elem, function(x) {x$getElementAttribute("data-value")})[[length(elem_data_values)]])
unlist(lapply(elem, function(x) {x$getElementAttribute("data-value")})
lapply(elem, function(x) {x$getElementAttribute("data-value")})
elem_data_values <- remDr$findElements(using = "css", value = ".pagination .action")
page_max         <- unlist(lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")})[[length(elem_data_values)]])
page_max
lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")
lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")})
elem_data_values <- remDr$findElements(using = "css", value = ".pagination .overview .action")
page_max         <- unlist(lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")})[[length(elem_data_values)]])
page_max
elem_data_values <- remDr$findElements(using = "css", value = ".pagination .action")
remDr$open()
## Go to page (Initiation)
remDr$navigate("http://nexisuni.com")
### Do a search: ENTER search terms
url_search <- make_url(search_terms = "Tafel and (Essen or Lebensmittel)",
time_from    = "01/01/1900",
time_to      = "31/01/1900")
remDr$navigate(url_search)
num_results_elem <- remDr$findElement(using = "css", value = ".resultsHeader")
num_results      <- num_results_elem$getElementText() %>%
unlist() %>%
str_extract("(?<=News \\().+(?=\\))") %>%
as.numeric()
num_results
### Do a search: ENTER search terms
url_search <- make_url(search_terms = "Tafel and (Essen or Lebensmittel)",
time_from    = "01/01/2000",
time_to      = "31/01/2000")
remDr$navigate(url_search)
elem_data_values <- remDr$findElements(using = "css", value = ".pagination .action")
page_max         <- unlist(lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")})[[length(elem_data_values)]])
page_source <- unlist(remDr$getPageSource())
html        <- read_html(page_source)
docids_lst  <- html %>% html_nodes(css = ".nexisuni-result-list input") %>% html_attr("data-docid") %>% str_replace("urn:contentItem:", "")
ids_lst     <- append(ids_lst, docids_lst)
ids_lst
docids_vec  <- html %>% html_nodes(css = ".nexisuni-result-list input") %>% html_attr("data-docid") %>% str_replace("urn:contentItem:", "")
docids_vec
### Do a search: ENTER search terms
url_search <- make_url(search_terms = "Tafel and (Essen or Lebensmittel)",
time_from    = "01/01/2000",
time_to      = "31/02/2000")
remDr$navigate(url_search)
### Do a search: ENTER search terms
url_search <- make_url(search_terms = "Tafel and (Essen or Lebensmittel)",
time_from    = "01/01/2000",
time_to      = "31/03/2000")
remDr$navigate(url_search)
elem_data_values <- remDr$findElements(using = "css", value = ".pagination .action")
page_max         <- unlist(lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")})[[length(elem_data_values)]])
page_max
elem_data_values <- remDr$findElements(using = "css", value = ".pagination .overview .action")
page_max         <- unlist(lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")})[[length(elem_data_values)]])
page_max
elem_data_values <- remDr$findElements(using = "css", value = ".pagination")
page_max         <- unlist(lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")})[[length(elem_data_values)]])
page_max
elem_data_values <- remDr$findElements(using = "css", value = ".action")
page_max         <- unlist(lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")})[[length(elem_data_values)]])
page_max
elem_data_values <- remDr$findElements(using = "css", value = ".action ")
elem_data_values
page_max         <- unlist(lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")})[[length(elem_data_values)]])
page_max
elem_data_values <- remDr$findElements(using = "css", value = ".action ")
page_max         <- unlist(lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")})
s <- [[length(elem_data_values)]])
page_max         <- unlist(lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")}))
page_max
elem_data_values <- remDr$findElements(using = "css", value = ".pagination .action ")
page_max         <- unlist(lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")}))
page_max
page_max         <- unlist(lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")}))[[length(elem_data_values)]]
elem_data_values
elem_data_values <- remDr$findElements(using = "css", value = ".pagination .action ")
pages            <- unlist(lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")}))
page_max         <- pages[[ength(elem_data_values)]]
page_max         <- pages[[length(elem_data_values)]]
pages
page_max         <- pages[[length(pages)]]
page_max
crawl_year <- function(time_from, time_to) {
### Navigate to page (search results)
url_search <- make_url(search_terms = "Tafel and (Essen or Lebensmittel)",
time_from    = time_from,
time_to      = time_to)
remDr$navigate(url_search)
# Extract number of results
num_results_elem <- remDr$findElement(using = "css", value = ".resultsHeader")
num_results      <- num_results_elem$getElementText() %>%
unlist() %>%
str_extract("(?<=News \\().+(?=\\))") %>%
as.numeric()
if (num_results < 11) {
# 0 --> return empty list
if (num_results == 0) {
return( c() )
} else {
# Extract All ids
page_source <- unlist(remDr$getPageSource())
html        <- read_html(page_source)
docids_vec  <- html %>% html_nodes(css = ".nexisuni-result-list input") %>% html_attr("data-docid") %>% str_replace("urn:contentItem:", "")
return(docids_vec)
}
# num_results > 10
} else {
# Get number of result pages
elem_data_values <- remDr$findElements(using = "css", value = ".pagination .action ")
pages            <- unlist(lapply(elem_data_values, function(x) {x$getElementAttribute("data-value")}))
page_max         <- pages[[length(pages)]]
# Create empty list
ids_vec <- c()
for (i in 1:(page_max-1)) {
### After all 10 results
if (i > 1) {
button_next <- remDr$findElement(using = "css", "a.la-TriangleRight")
button_next$clickElement()
Sys.sleep(1)
}
## Get Ids for the search result
page_source <- unlist(remDr$getPageSource())
html        <- read_html(page_source)
docids  <- html %>% html_nodes(css = ".nexisuni-result-list input") %>% html_attr("data-docid") %>% str_replace("urn:contentItem:", "")
ids_vec <- append(ids_vec, docids)
print(i)
}
return(ids_lst)
}
}
result <- crawl_year(time_from = "01/01/2000", time_to = "31/03/2000")
page_max
pages            <- remDr$findElements(using = "css", value = ".pagination .action ") %>%
map( unction(x) {x$getElementAttribute("data-value")} ) %>%
unlist()
pages            <- remDr$findElements(using = "css", value = ".pagination .action ") %>%
map( unction(x) {x$getElementAttribute("data-value")} )
pages            <- remDr$findElements(using = "css", value = ".pagination .action ") %>%
map( function(x) {x$getElementAttribute("data-value")} )
library(purrr)
pages            <- remDr$findElements(using = "css", value = ".pagination .action ") %>%
map( function(x) {x$getElementAttribute("data-value")} )
pages
pages            <- remDr$findElements(using = "css", value = ".pagination .action ") %>%
map( function(x) {x$getElementAttribute("data-value")} ) %>%
unlist()
pages
pages            <- remDr$findElements(using = "css", value = ".pagination .action ") %>%
map( function(x) {x$getElementAttribute("data-value")} ) %>%
unlist() %>%
.[[length(.)]]
pages
page_max <- remDr$findElements(using = "css", value = ".pagination .action ") %>%
map( function(x) {x$getElementAttribute("data-value")} ) %>%
unlist() %>%
.[[length(.)]] %>%
as.numeric()
page_max
crawl_year <- function(time_from, time_to) {
### Navigate to page (search results)
url_search <- make_url(search_terms = "Tafel and (Essen or Lebensmittel)",
time_from    = time_from,
time_to      = time_to)
remDr$navigate(url_search)
# Extract number of results
num_results_elem <- remDr$findElement(using = "css", value = ".resultsHeader")
num_results      <- num_results_elem$getElementText() %>%
unlist() %>%
str_extract("(?<=News \\().+(?=\\))") %>%
as.numeric()
if (num_results < 11) {
# 0 --> return empty list
if (num_results == 0) {
return( c() )
} else {
# Extract All ids
page_source <- unlist(remDr$getPageSource())
html        <- read_html(page_source)
docids_vec  <- html %>% html_nodes(css = ".nexisuni-result-list input") %>% html_attr("data-docid") %>% str_replace("urn:contentItem:", "")
return(docids_vec)
}
# num_results > 10
} else {
# Get number of result pages
page_max <- remDr$findElements(using = "css", value = ".pagination .action ") %>%
map( function(x) {x$getElementAttribute("data-value")} ) %>%
unlist() %>%
.[[length(.)]] %>%
as.numeric()
# Create empty list
ids_vec <- c()
for (i in 1:(page_max-1)) {
### After all 10 results
if (i > 1) {
button_next <- remDr$findElement(using = "css", "a.la-TriangleRight")
button_next$clickElement()
Sys.sleep(1)
}
## Get Ids for the search result
page_source <- unlist(remDr$getPageSource())
html        <- read_html(page_source)
docids  <- html %>% html_nodes(css = ".nexisuni-result-list input") %>% html_attr("data-docid") %>% str_replace("urn:contentItem:", "")
ids_vec <- append(ids_vec, docids)
print(i)
}
return(ids_lst)
}
}
result <- crawl_year(time_from = "01/01/2000", time_to = "31/03/2000")
result
crawl_year <- function(time_from, time_to) {
### Navigate to page (search results)
url_search <- make_url(search_terms = "Tafel and (Essen or Lebensmittel)",
time_from    = time_from,
time_to      = time_to)
remDr$navigate(url_search)
# Extract number of results
num_results_elem <- remDr$findElement(using = "css", value = ".resultsHeader")
num_results      <- num_results_elem$getElementText() %>%
unlist() %>%
str_extract("(?<=News \\().+(?=\\))") %>%
as.numeric()
if (num_results < 11) {
# 0 --> return empty list
if (num_results == 0) {
return( c() )
} else {
# Extract All ids
page_source <- unlist(remDr$getPageSource())
html        <- read_html(page_source)
docids_vec  <- html %>% html_nodes(css = ".nexisuni-result-list input") %>% html_attr("data-docid") %>% str_replace("urn:contentItem:", "")
return(docids_vec)
}
# num_results > 10
} else {
# Get number of result pages
page_max <- remDr$findElements(using = "css", value = ".pagination .action ") %>%
map( function(x) {x$getElementAttribute("data-value")} ) %>%
unlist() %>%
.[[length(.)]] %>%
as.numeric()
# Create empty list
ids_vec <- c()
for (i in 1:(page_max-1)) {
### After all 10 results
if (i > 1) {
button_next <- remDr$findElement(using = "css", "a.la-TriangleRight")
button_next$clickElement()
Sys.sleep(1)
}
## Get Ids for the search result
page_source <- unlist(remDr$getPageSource())
html        <- read_html(page_source)
docids  <- html %>% html_nodes(css = ".nexisuni-result-list input") %>% html_attr("data-docid") %>% str_replace("urn:contentItem:", "")
ids_vec <- append(ids_vec, docids)
print(i)
}
return(ids_vec)
}
}
result <- crawl_year(time_from = "01/01/2000", time_to = "31/03/2000")
result
crawl_year <- function(time_from, time_to) {
### Navigate to page (search results)
url_search <- make_url(search_terms = "Tafel and (Essen or Lebensmittel)",
time_from    = time_from,
time_to      = time_to)
remDr$navigate(url_search)
# Extract number of results
num_results_elem <- remDr$findElement(using = "css", value = ".resultsHeader")
num_results      <- num_results_elem$getElementText() %>%
unlist() %>%
str_extract("(?<=News \\().+(?=\\))") %>%
as.numeric()
if (num_results < 11) {
# 0 --> return empty list
if (num_results == 0) {
return( c() )
} else {
# Extract All ids
page_source <- unlist(remDr$getPageSource())
html        <- read_html(page_source)
docids_vec  <- html %>% html_nodes(css = ".nexisuni-result-list input") %>% html_attr("data-docid") %>% str_replace("urn:contentItem:", "")
return(docids_vec)
}
# num_results > 10
} else {
# Get number of result pages
page_max <- remDr$findElements(using = "css", value = ".pagination .action ") %>%
map( function(x) {x$getElementAttribute("data-value")} ) %>%
unlist() %>%
.[[length(.)]] %>%
as.numeric()
# Create empty list
ids_vec <- c()
for (i in 1:page_max) {
### After all 10 results
if (i > 1) {
button_next <- remDr$findElement(using = "css", "a.la-TriangleRight")
button_next$clickElement()
Sys.sleep(1)
}
## Get Ids for the search result
page_source <- unlist(remDr$getPageSource())
html        <- read_html(page_source)
docids  <- html %>% html_nodes(css = ".nexisuni-result-list input") %>% html_attr("data-docid") %>% str_replace("urn:contentItem:", "")
ids_vec <- append(ids_vec, docids)
print(i)
}
return(ids_vec)
}
}
result <- crawl_year(time_from = "01/01/2000", time_to = "31/03/2000")
result
remDr$findElement(using = "css", value = ".resultsHeader") %>%
.$getElementText()
num_results_elem[[1]]
class(num_results_elem)
remDr$findElement(using = "css", value = ".resultsHeader") %>%
"$"getElementText()
num_results <- remDr$findElement(using = "css", value = ".resultsHeader") %>%
(.$getElementText())
# 2. Extract number of results
num_results <- remDr$findElement(using = "css", value = ".resultsHeader") %>%
`.`$getElementText()
typeof(num_results_elem)
num_results_elem@.xData
# 2. Extract number of results
num_results <- remDr$findElement(using = "css", value = ".resultsHeader") %>%
.$getElementText()
install.packages("seleniumPipes")
library(seleniumPipes)
remDr %>%
findElement(using = "css", value = ".resultsHeader") %>%
getElementText()
remDr$close()
driver$server$stop()
driver <- rsDriver(browser = "firefox", extraCapabilities = fprof)
remDr  <- driver$client
## Go to page (Initiation)
remDr %>% go("http://nexisuni.com")
remDr$close()
driver$server$stop()
remDr <- remoteDr(browserName = "firefox", extraCapabilities = fprof)
# Startup Selenium Browser
# driver <- rsDriver(browser = "firefox", extraCapabilities = fprof)
remDr <- remoteDr(browserName = "firefox", extraCapabilities = fprof, port = 4567L)
remoteDr
# Startup Selenium Browser
# driver <- rsDriver(browser = "firefox", extraCapabilities = fprof)
remDr <- remoteDr(browserName = "firefox", extraCapabilities = fprof, port = 4567L)
# Startup Selenium Browser
driver <- rsDriver(browser = "firefox", extraCapabilities = fprof)
remDr  <- driver$client
## Go to page (Initiation)
remDr %>% go("http://nexisuni.com")
remDr  <- driver$client
remDr$navigate("http://nexisuni.com")
remDr %>%
findElement(using = "css", value = ".resultsHeader") %>%
getElementText()
remDr$close()
driver$server$stop()
remDr  <- remoteDr(browserName = "firefox", extraCapabilities = fprof, port = 4567L)
remDr <- remoteDr(browserName = "chrome")
remDr <- remoteDr(browserName = "chrome", port = 123L)
remoteDr
newSession
parse_url
parse_url("http://localhost")
curl::curl_fetch_memory("https://httpbin.org/get", new_handle(verbose = TRUE))
library(curl)
curl_fetch_memory("https://httpbin.org/get", new_handle(verbose = TRUE))
remDr  <- remoteDr(browserName = "firefox", extraCapabilities = fprof, port = 4444L)
file.path(find.package("RSelenium"), "examples/serverUtils")
remDr$close()
driver$server$stop()
